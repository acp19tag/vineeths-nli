{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: This is broken. \n",
    "\n",
    "The code was developed using some earlier version of BERTModels and it is incredibly time consuming to fix it - would require going into the DL2 package and updating there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                          num_heads, num_layers, dropout, max_len, key_size, query_size, value_size,\n",
    "                          hid_in_features, mlm_in_features, nsp_in_features):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # Define an empty vocabulary to load the predefined vocabulary\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "        vocab.idx_to_token)}\n",
    "    \n",
    "    # print(f'len vocab: {len(vocab)}')\n",
    "    \n",
    "    bert = d2l.BERTModel(\n",
    "        vocab_size = len(vocab), \n",
    "        num_hiddens = num_hiddens,\n",
    "        norm_shape = norm_shape,\n",
    "        ffn_num_input = ffn_num_input, \n",
    "        ffn_num_hiddens=ffn_num_hiddens, \n",
    "        num_heads = num_heads,\n",
    "        num_layers = num_layers,\n",
    "        dropout=dropout, \n",
    "        max_len=max_len,\n",
    "        key_size=key_size,\n",
    "        query_size=query_size,\n",
    "        value_size=value_size,\n",
    "        hid_in_features=hid_in_features,\n",
    "        mlm_in_features=mlm_in_features,\n",
    "        nsp_in_features=nsp_in_features\n",
    "        )\n",
    "    \n",
    "    # Load pretrained BERT parameters\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir,\n",
    "                                                 'pretrained.params')))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'bert.base'\n",
    "# num_hiddens = 768\n",
    "# norm_shape = [768]\n",
    "# ffn_num_input = 768\n",
    "# ffn_num_hiddens = 3072\n",
    "# num_heads = 12\n",
    "# num_layers = 2\n",
    "# dropout = 0.2\n",
    "\n",
    "# devices = d2l.try_all_gpus()\n",
    "# max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert.small'\n",
    "num_hiddens = 256 # not it\n",
    "norm_shape = [256] # not it\n",
    "ffn_num_input = 256 # not it\n",
    "ffn_num_hiddens = 512 # not it\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "key_size=256,\n",
    "query_size=256,\n",
    "value_size=256,\n",
    "hid_in_features=256,\n",
    "mlm_in_features=256,\n",
    "nsp_in_features=256\n",
    "\n",
    "devices = d2l.try_all_gpus()\n",
    "max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bert, vocab \u001b[39m=\u001b[39m load_pretrained_model(\n\u001b[1;32m      2\u001b[0m     pretrained_model \u001b[39m=\u001b[39;49m model_name, \n\u001b[1;32m      3\u001b[0m     num_hiddens \u001b[39m=\u001b[39;49m num_hiddens, \n\u001b[1;32m      4\u001b[0m     norm_shape\u001b[39m=\u001b[39;49mnorm_shape, \n\u001b[1;32m      5\u001b[0m     ffn_num_input\u001b[39m=\u001b[39;49mffn_num_input, \n\u001b[1;32m      6\u001b[0m     ffn_num_hiddens\u001b[39m=\u001b[39;49mffn_num_hiddens,\n\u001b[1;32m      7\u001b[0m     num_heads\u001b[39m=\u001b[39;49mnum_heads, \n\u001b[1;32m      8\u001b[0m     num_layers\u001b[39m=\u001b[39;49mnum_layers, \n\u001b[1;32m      9\u001b[0m     dropout\u001b[39m=\u001b[39;49mdropout, \n\u001b[1;32m     10\u001b[0m     max_len\u001b[39m=\u001b[39;49mmax_len,\n\u001b[1;32m     11\u001b[0m     key_size\u001b[39m=\u001b[39;49mkey_size,\n\u001b[1;32m     12\u001b[0m     query_size\u001b[39m=\u001b[39;49mquery_size,\n\u001b[1;32m     13\u001b[0m     value_size\u001b[39m=\u001b[39;49mvalue_size,\n\u001b[1;32m     14\u001b[0m     hid_in_features\u001b[39m=\u001b[39;49mhid_in_features,\n\u001b[1;32m     15\u001b[0m     mlm_in_features\u001b[39m=\u001b[39;49mmlm_in_features,\n\u001b[1;32m     16\u001b[0m     nsp_in_features\u001b[39m=\u001b[39;49mnsp_in_features\n\u001b[1;32m     17\u001b[0m )\n",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m, in \u001b[0;36mload_pretrained_model\u001b[0;34m(pretrained_model, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, key_size, query_size, value_size, hid_in_features, mlm_in_features, nsp_in_features)\u001b[0m\n\u001b[1;32m      8\u001b[0m vocab\u001b[39m.\u001b[39mtoken_to_idx \u001b[39m=\u001b[39m {token: idx \u001b[39mfor\u001b[39;00m idx, token \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\n\u001b[1;32m      9\u001b[0m     vocab\u001b[39m.\u001b[39midx_to_token)}\n\u001b[1;32m     11\u001b[0m \u001b[39m# print(f'len vocab: {len(vocab)}')\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m bert \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39;49mBERTModel(\n\u001b[1;32m     14\u001b[0m     vocab_size \u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(vocab), \n\u001b[1;32m     15\u001b[0m     num_hiddens \u001b[39m=\u001b[39;49m num_hiddens,\n\u001b[1;32m     16\u001b[0m     norm_shape \u001b[39m=\u001b[39;49m norm_shape,\n\u001b[1;32m     17\u001b[0m     ffn_num_input \u001b[39m=\u001b[39;49m ffn_num_input, \n\u001b[1;32m     18\u001b[0m     ffn_num_hiddens\u001b[39m=\u001b[39;49mffn_num_hiddens, \n\u001b[1;32m     19\u001b[0m     num_heads \u001b[39m=\u001b[39;49m num_heads,\n\u001b[1;32m     20\u001b[0m     num_layers \u001b[39m=\u001b[39;49m num_layers,\n\u001b[1;32m     21\u001b[0m     dropout\u001b[39m=\u001b[39;49mdropout, \n\u001b[1;32m     22\u001b[0m     max_len\u001b[39m=\u001b[39;49mmax_len,\n\u001b[1;32m     23\u001b[0m     key_size\u001b[39m=\u001b[39;49mkey_size,\n\u001b[1;32m     24\u001b[0m     query_size\u001b[39m=\u001b[39;49mquery_size,\n\u001b[1;32m     25\u001b[0m     value_size\u001b[39m=\u001b[39;49mvalue_size,\n\u001b[1;32m     26\u001b[0m     hid_in_features\u001b[39m=\u001b[39;49mhid_in_features,\n\u001b[1;32m     27\u001b[0m     mlm_in_features\u001b[39m=\u001b[39;49mmlm_in_features,\n\u001b[1;32m     28\u001b[0m     nsp_in_features\u001b[39m=\u001b[39;49mnsp_in_features\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[39m# Load pretrained BERT parameters\u001b[39;00m\n\u001b[1;32m     32\u001b[0m bert\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir,\n\u001b[1;32m     33\u001b[0m                                              \u001b[39m'\u001b[39m\u001b[39mpretrained.params\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "File \u001b[0;32m~/miniconda3/envs/vineeths2/lib/python3.10/site-packages/d2l/torch.py:2327\u001b[0m, in \u001b[0;36mBERTModel.__init__\u001b[0;34m(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, key_size, query_size, value_size, hid_in_features, mlm_in_features, nsp_in_features)\u001b[0m\n\u001b[1;32m   2321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n\u001b[1;32m   2322\u001b[0m              ffn_num_hiddens, num_heads, num_layers, dropout,\n\u001b[1;32m   2323\u001b[0m              max_len\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, key_size\u001b[39m=\u001b[39m\u001b[39m768\u001b[39m, query_size\u001b[39m=\u001b[39m\u001b[39m768\u001b[39m, value_size\u001b[39m=\u001b[39m\u001b[39m768\u001b[39m,\n\u001b[1;32m   2324\u001b[0m              hid_in_features\u001b[39m=\u001b[39m\u001b[39m768\u001b[39m, mlm_in_features\u001b[39m=\u001b[39m\u001b[39m768\u001b[39m,\n\u001b[1;32m   2325\u001b[0m              nsp_in_features\u001b[39m=\u001b[39m\u001b[39m768\u001b[39m):\n\u001b[1;32m   2326\u001b[0m     \u001b[39msuper\u001b[39m(BERTModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m-> 2327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m BERTEncoder(vocab_size, num_hiddens, norm_shape,\n\u001b[1;32m   2328\u001b[0m                 ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n\u001b[1;32m   2329\u001b[0m                 dropout, max_len\u001b[39m=\u001b[39;49mmax_len, key_size\u001b[39m=\u001b[39;49mkey_size,\n\u001b[1;32m   2330\u001b[0m                 query_size\u001b[39m=\u001b[39;49mquery_size, value_size\u001b[39m=\u001b[39;49mvalue_size)\n\u001b[1;32m   2331\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(nn\u001b[39m.\u001b[39mLinear(hid_in_features, num_hiddens),\n\u001b[1;32m   2332\u001b[0m                                 nn\u001b[39m.\u001b[39mTanh())\n\u001b[1;32m   2333\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlm \u001b[39m=\u001b[39m MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
      "File \u001b[0;32m~/miniconda3/envs/vineeths2/lib/python3.10/site-packages/d2l/torch.py:2264\u001b[0m, in \u001b[0;36mBERTEncoder.__init__\u001b[0;34m(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len, key_size, query_size, value_size, **kwargs)\u001b[0m\n\u001b[1;32m   2262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential()\n\u001b[1;32m   2263\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_layers):\n\u001b[0;32m-> 2264\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblks\u001b[39m.\u001b[39madd_module(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, d2l\u001b[39m.\u001b[39;49mEncoderBlock(\n\u001b[1;32m   2265\u001b[0m         key_size, query_size, value_size, num_hiddens, norm_shape,\n\u001b[1;32m   2266\u001b[0m         ffn_num_input, ffn_num_hiddens, num_heads, dropout, \u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m   2267\u001b[0m \u001b[39m# In BERT, positional embeddings are learnable, thus we create a\u001b[39;00m\n\u001b[1;32m   2268\u001b[0m \u001b[39m# parameter of positional embeddings that are long enough\u001b[39;00m\n\u001b[1;32m   2269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, max_len,\n\u001b[1;32m   2270\u001b[0m                                               num_hiddens))\n",
      "File \u001b[0;32m~/miniconda3/envs/vineeths2/lib/python3.10/site-packages/d2l/torch.py:1339\u001b[0m, in \u001b[0;36mEncoderBlock.__init__\u001b[0;34m(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias, **kwargs)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, key_size, query_size, value_size, num_hiddens,\n\u001b[1;32m   1336\u001b[0m              norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n\u001b[1;32m   1337\u001b[0m              dropout, use_bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1338\u001b[0m     \u001b[39msuper\u001b[39m(EncoderBlock, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1339\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39;49mMultiHeadAttention(\n\u001b[1;32m   1340\u001b[0m         key_size, query_size, value_size, num_hiddens, num_heads, dropout,\n\u001b[1;32m   1341\u001b[0m         use_bias)\n\u001b[1;32m   1342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maddnorm1 \u001b[39m=\u001b[39m AddNorm(norm_shape, dropout)\n\u001b[1;32m   1343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn \u001b[39m=\u001b[39m PositionWiseFFN(\n\u001b[1;32m   1344\u001b[0m         ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
      "File \u001b[0;32m~/miniconda3/envs/vineeths2/lib/python3.10/site-packages/d2l/torch.py:1224\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[0;34m(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias, **kwargs)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads \u001b[39m=\u001b[39m num_heads\n\u001b[1;32m   1223\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39mDotProductAttention(dropout)\n\u001b[0;32m-> 1224\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_q \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(query_size, num_hiddens, bias\u001b[39m=\u001b[39;49mbias)\n\u001b[1;32m   1225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_k \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(key_size, num_hiddens, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m   1226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_v \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(value_size, num_hiddens, bias\u001b[39m=\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/vineeths2/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2"
     ]
    }
   ],
   "source": [
    "bert, vocab = load_pretrained_model(\n",
    "    pretrained_model = model_name, \n",
    "    num_hiddens = num_hiddens, \n",
    "    norm_shape=norm_shape, \n",
    "    ffn_num_input=ffn_num_input, \n",
    "    ffn_num_hiddens=ffn_num_hiddens,\n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout, \n",
    "    max_len=max_len,\n",
    "    key_size=key_size,\n",
    "    query_size=query_size,\n",
    "    value_size=value_size,\n",
    "    hid_in_features=hid_in_features,\n",
    "    mlm_in_features=mlm_in_features,\n",
    "    nsp_in_features=nsp_in_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vineeths-nli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
